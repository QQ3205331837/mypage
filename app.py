from flask import Flask, render_template, request, jsonify
from flask import Response, send_from_directory
import csv
import io
import requests
import os
import time
import random
from urllib.parse import quote_plus
from collections import defaultdict
from bs4 import BeautifulSoup
from datetime import datetime
import re
from markupsafe import Markup
import logging

# é…ç½®æ—¥å¿— - é€‚é…Vercelæ— æœåŠ¡å™¨ç¯å¢ƒ
def setup_logging():
    """Vercelç¯å¢ƒé€‚é…çš„æ—¥å¿—é…ç½®"""
    import os
    
    # Vercelç¯å¢ƒå˜é‡
    is_vercel = os.environ.get('VERCEL') or os.environ.get('NOW_REGION')
    
    if is_vercel:
        # Vercelç¯å¢ƒï¼šç®€åŒ–æ—¥å¿—ï¼Œé¿å…æ–‡ä»¶æ“ä½œ
        logging.basicConfig(
            level=logging.INFO,  # Vercelç”Ÿäº§ç¯å¢ƒä½¿ç”¨INFOçº§åˆ«
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )
    else:
        # æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šè¯¦ç»†æ—¥å¿—
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )

setup_logging()
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder='static', static_url_path='/static')

# Vercelé€‚é…ï¼šä¼˜åŒ–é™æ€æ–‡ä»¶æœåŠ¡
@app.route('/static/image/<path:filename>')
def serve_static_image(filename):
    """ç›´æ¥æœåŠ¡static/imageç›®å½•ä¸‹çš„å›¾ç‰‡æ–‡ä»¶"""
    static_folder = app.static_folder or 'static'
    return send_from_directory(os.path.join(static_folder, 'image'), filename)

# å…¼å®¹æ—§è·¯å¾„ï¼š/image/ -> /static/image/
@app.route('/image/<path:filename>')
def serve_image(filename):
    """å…¼å®¹æ—§å›¾ç‰‡è·¯å¾„ï¼Œé‡å®šå‘åˆ°æ–°çš„é™æ€è·¯å¾„"""
    static_folder = app.static_folder or 'static'
    return send_from_directory(os.path.join(static_folder, 'image'), filename)

# ç®€å•å†…å­˜ç¼“å­˜ï¼škey=(website), value={"data": list, "ts": epoch_seconds}
CACHE_TTL_SECONDS = 600  # 10åˆ†é’Ÿ
_cache = {}

# è§‚æµ‹ï¼šæŠ“å–æŒ‡æ ‡ä¸é”™è¯¯æ—¥å¿—ï¼ˆå†…å­˜ï¼‰
_metrics = {
    'last_fetch': {},  # website -> {ts, duration_ms, count}
}
_errors = []  # [{ts, website, stage, message}]

def _check_admin_key():
    expected = os.environ.get('ADMIN_KEY', '').strip()
    if not expected:
        return True  # æœªè®¾ç½®å³ä¸é‰´æƒ
    supplied = request.args.get('key') or request.headers.get('X-Admin-Key')
    return supplied == expected

# ç½‘ç«™é…ç½®ä¿¡æ¯
websites = {
    "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘": {
        "url": "https://www.ctnews.com.cn/jujiao/node_1823.html",
        "link_css": 'a[href*="/content/"]',
        "base_url": "https://www.ctnews.com.cn",
        "date_pattern": r"/content/(\\d{4}-\\d{2})/(\\d{2})/"
    },
    "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“": {
        "url": "http://travel.people.com.cn/",
        "link_css": [
            "ul.list_16 a"
        ],
        "content_css": ".article-content, #article_body",
        "base_url": "http://travel.people.com.cn/",
        "date_pattern": "(\\d{4})-(\\d{2})-(\\d{2})",
        "special_handler": False
    },
    "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“": {
        "url": "https://travel.cnr.cn/",
        "link_css": 'a[href]:not([href*="javascript"]):not([href*="#"])',  # å®½æ¾é€‰æ‹©å™¨
        "base_url": "https://travel.cnr.cn",
        "date_pattern": None
    }
}

# å…¬å¸å‘¨æŠ¥é…ç½®
WECHAT_ALBUM_URL = "https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4ODA2ODMzNA==&action=getalbum&album_id=4180740440766726147#wechat_redirect"

# å…¬å¸å‘¨æŠ¥æ•°æ®ï¼ˆä»å¾®ä¿¡ä¸“æ å®é™…æŠ“å–ï¼‰
company_reports = []

def get_wechat_reports():
    """è·å–å¾®ä¿¡ä¸“æ çš„å…¬å¸å‘¨æŠ¥å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        from enhanced_reports import enhanced_wechat_reports
        reports = enhanced_wechat_reports()
        return reports
        
    except Exception as e:
        logger.error(f"è·å–å¾®ä¿¡ä¸“æ å¤±è´¥: {str(e)}")
        return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        try:
            with open('c:\\Users\\zhao\\Desktop\\pyLy\\news_web\\wechat_album.html', 'r', encoding='utf-8') as f:
                local_html = f.read()
            
            # è°ƒè¯•ï¼šæ‰“å°æ–‡ä»¶å†…å®¹ç‰‡æ®µ
            print("=== å¾®ä¿¡ä¸“æ HTMLæ–‡ä»¶å†…å®¹ç‰‡æ®µ ===")
            print(local_html[:1000])
            print("==============================")
            
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
            # å…ˆæ‰¾åˆ°æ‰€æœ‰liå…ƒç´ ï¼Œç„¶ååœ¨æ¯ä¸ªliå…ƒç´ ä¸­æå–data-titleå’Œdata-link
            li_pattern = r'<li[^>]*?class="album__list-item[\s\S]*?</li>'
            li_matches = re.findall(li_pattern, local_html)
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(li_matches)} ä¸ªliå…ƒç´ ")
            
            matches = []
            for li_content in li_matches:
                # åœ¨æ¯ä¸ªliå…ƒç´ ä¸­æå–data-titleå’Œdata-link
                title_match = re.search(r'data-title="([^"]+)"', li_content)
                link_match = re.search(r'data-link="([^"]+)"', li_content)
                
                if title_match and link_match:
                    matches.append((title_match.group(1), link_match.group(1)))
            
            print(f"=== æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç»“æœ ===")
            print(f"æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…é¡¹")
            for i, match in enumerate(matches[:3]):
                print(f"åŒ¹é…é¡¹ {i+1}: æ ‡é¢˜='{match[0]}', é“¾æ¥='{match[1]}'")
            print("========================")
            
            for match in matches:
                if len(match) >= 2:
                    title = str(match[0])  # ç¡®ä¿titleæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    link = str(match[1])     # ç¡®ä¿linkæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    
                    # ä¿®å¤ç¼–ç é—®é¢˜ï¼šå°† \x26amp; å’Œ &amp; æ›¿æ¢ä¸º &
                    title = title.replace('\x26amp;', '&').replace('&amp;', '&')
                    
                    # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                    if link and not link.startswith('http'):
                        link = 'https:' + link if link.startswith('//') else 'https://' + link
                    
                    if title and link and len(title) > 5:
                        # ç”Ÿæˆåˆç†çš„å‘å¸ƒæ—¥æœŸï¼ˆä»æœ€æ–°åˆ°æœ€æ—§ï¼‰
                        base_date = datetime.now()
                        date_offset = len(reports) * 7  # æ¯å‘¨ä¸€ç¯‡
                        report_date = (base_date - timedelta(days=date_offset)).strftime('%Y-%m-%d')
                        
                        reports.append({
                            "title": title.strip(),
                            "link": link.strip(),
                            "date": report_date,
                            "source": "å…¬å¸å‘¨æŠ¥"
                        })
        except Exception as e:
            print(f"è¯»å–æœ¬åœ°HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿æŠ“å–
        if not reports:
            try:
                url = "https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA4ODA2ODMzNA==&action=getalbum&album_id=4180740440766726147#wechat_redirect"
                response = requests.get(url, timeout=10)
                html_content = response.text
                
                # ä»åœ¨çº¿HTMLä¸­æå–æ ‡é¢˜å’Œé“¾æ¥
                article_patterns = [
                    r'data-title="([^"]+)".*?data-link="([^"]+)"',
                    r'data-title="([^"]+)"[^>]*data-link="([^"]+)"',
                ]
                
                for pattern in article_patterns:
                    matches = re.findall(pattern, html_content, re.DOTALL)
                    if matches:
                        for match in matches:
                            if len(match) >= 2:
                                title = str(match[0])  # ç¡®ä¿titleæ˜¯å­—ç¬¦ä¸²ç±»å‹
                                link = str(match[1])     # ç¡®ä¿linkæ˜¯å­—ç¬¦ä¸²ç±»å‹
                                
                                # ä¿®å¤ç¼–ç é—®é¢˜ï¼šå°† \x26amp; å’Œ &amp; æ›¿æ¢ä¸º &
                                title = title.replace('\x26amp;', '&').replace('&amp;', '&')
                                
                                # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                                if link and not link.startswith('http'):
                                    link = 'https:' + link if link.startswith('//') else 'https://' + link
                                
                                if title and link and len(title) > 5:
                                    # ç”Ÿæˆåˆç†çš„å‘å¸ƒæ—¥æœŸï¼ˆä»æœ€æ–°åˆ°æœ€æ—§ï¼‰
                                    base_date = datetime.now()
                                    date_offset = len(reports) * 7  # æ¯å‘¨ä¸€ç¯‡
                                    report_date = (base_date - timedelta(days=date_offset)).strftime('%Y-%m-%d')
                                    
                                    reports.append({
                                        "title": title.strip(),
                                        "link": link.strip(),
                                        "date": report_date,
                                        "source": "å…¬å¸å‘¨æŠ¥"
                                    })
                        
                        if reports:
                            break
            except Exception as e:
                print(f"åœ¨çº¿æŠ“å–å¤±è´¥: {str(e)}")
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not reports:
            reports = []
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print("=== å…¬å¸å‘¨æŠ¥æ•°æ®æŠ“å–ç»“æœ ===")
        print(f"æŠ“å–åˆ°çš„å‘¨æŠ¥æ•°é‡: {len(reports)}")
        for i, report in enumerate(reports):
            print(f"å‘¨æŠ¥ {i+1}: {repr(report.get('title', ''))}")
            print(f"     é“¾æ¥: {report.get('link', '')}")
        print("==========================")
        
        return reports
        
    except Exception as e:
        logger.error(f"è·å–å¾®ä¿¡ä¸“æ å¤±è´¥: {str(e)}")
        return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä½¿ç”¨ç¤ºä¾‹æ•°æ®

ALL_SOURCES_LABEL = "å…¨éƒ¨æ¥æº"

@app.route('/')
def index():
    # è·å–é€‰é¡¹å¡ç±»å‹ï¼šnewsï¼ˆå›½å†…æ–°é—»ï¼‰æˆ– reportsï¼ˆå…¬å¸å‘¨æŠ¥ï¼‰
    tab_type = request.args.get('tab', 'news')
    
    # å¦‚æœæ˜¯æ–°é—»é€‰é¡¹å¡ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
    if tab_type == 'news':
        # é»˜è®¤æ˜¾ç¤ºäººæ°‘ç½‘æ—…æ¸¸é¢‘é“çš„æ–°é—»
        website = request.args.get('website', 'äººæ°‘ç½‘æ—…æ¸¸é¢‘é“')
        sources_str = request.args.get('sources', '').strip()
        selected_sources = [s for s in [x.strip() for x in sources_str.split(',')] if s] if sources_str else []
        search_text = request.args.get('search', '')
        
        # æ”¯æŒå¼ºåˆ¶åˆ·æ–° ?refresh=1
        refresh = request.args.get('refresh', '0') == '1'
        # è·å–æ–°é—»æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼Œæ”¯æŒèšåˆä¸å¤šæ¥æºé€‰æ‹©
        if selected_sources:
            valid_sources = [s for s in selected_sources if s in websites]
            aggregated = []
            for site in valid_sources:
                aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
            # å»é‡ï¼ˆæŒ‰é“¾æ¥ï¼‰
            seen = set()
            deduped = []
            for item in aggregated:
                link = item.get('link')
                if link and link not in seen:
                    seen.add(link)
                    deduped.append(item)
            deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
            news_data = deduped
        elif website == ALL_SOURCES_LABEL:
            # èšåˆæ‰€æœ‰æ¥æº
            aggregated = []
            for site in websites.keys():
                aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
            # å»é‡ï¼ˆæŒ‰é“¾æ¥ï¼‰
            seen = set()
            deduped = []
            for item in aggregated:
                link = item.get('link')
                if link and link not in seen:
                    seen.add(link)
                    deduped.append(item)
            # æŒ‰æ—¥æœŸå€’åº
            deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
            news_data = deduped
        else:
            news_data = get_news_with_cache(website, force_refresh=refresh)

        # å»é‡ï¼ˆæŒ‰é“¾æ¥ï¼‰ï¼Œå³ä½¿æ˜¯å•ä¸ªç½‘ç«™ä¹Ÿå¯èƒ½æœ‰é‡å¤
        seen = set()
        deduped = []
        for item in news_data:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        news_data = deduped

        # å…¬å…±è¿‡æ»¤
        news_data = filter_news(news_data, search_text)
        
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        
        # åˆ†é¡µå‚æ•°
        try:
            page = int(request.args.get('page', '1'))
            page_size = int(request.args.get('page_size', '12'))
            page = max(page, 1)
            page_size = max(min(page_size, 30), 6)
        except Exception:
            page, page_size = 1, 12

        start = (page - 1) * page_size
        end = start + page_size
        paged_news = news_data[start:end]
        total_pages = max((len(news_data) + page_size - 1) // page_size, 1)

        return render_template('index.html', 
                              news_data=paged_news, 
                              websites=[ALL_SOURCES_LABEL] + list(websites.keys()), 
                              current_website=website,
                              search_text=search_text,
                              selected_sources=','.join(selected_sources),
                              news_count=len(news_data),
                              page=page,
                              page_size=page_size,
                              total_pages=total_pages,
                              now=now,
                              metrics=_metrics['last_fetch'].get(website if website != ALL_SOURCES_LABEL else 'èšåˆ', None),
                              tab_type=tab_type,
                              company_reports=[])
    
    # å¦‚æœæ˜¯å…¬å¸å‘¨æŠ¥é€‰é¡¹å¡
    elif tab_type == 'reports':
        # è·å–å…¬å¸å‘¨æŠ¥æ•°æ®
        reports_data = get_wechat_reports()
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®é™…æŠ“å–åˆ°çš„æ•°æ®
        print("=== å…¬å¸å‘¨æŠ¥æ•°æ®è°ƒè¯•ä¿¡æ¯ ===")
        print(f"æŠ“å–åˆ°çš„å‘¨æŠ¥æ•°é‡: {len(reports_data)}")
        for i, report in enumerate(reports_data[:3]):
            print(f"å‘¨æŠ¥ {i+1}:")
            print(f"  æ ‡é¢˜: {repr(report.get('title', ''))}")
            print(f"  é“¾æ¥: {report.get('link', '')}")
            print(f"  æ—¥æœŸ: {report.get('date', '')}")
        print("==========================")
        
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        
        # åˆ†é¡µå‚æ•°
        try:
            page = int(request.args.get('page', '1'))
            page_size = int(request.args.get('page_size', '12'))
            page = max(page, 1)
            page_size = max(min(page_size, 30), 6)
        except Exception:
            page, page_size = 1, 12

        start = (page - 1) * page_size
        end = start + page_size
        paged_reports = reports_data[start:end]
        total_pages = max((len(reports_data) + page_size - 1) // page_size, 1)

        return render_template('index.html', 
                              news_data=[], 
                              websites=[ALL_SOURCES_LABEL] + list(websites.keys()), 
                              current_website='å…¬å¸å‘¨æŠ¥',
                              search_text='',
                              selected_sources='',
                              news_count=len(reports_data),
                              page=page,
                              page_size=page_size,
                              total_pages=total_pages,
                              now=now,
                              metrics=None,
                              tab_type=tab_type,
                              company_reports=paged_reports)

def filter_news(news_data, search_text):
    # æ·»åŠ è¿‡æ»¤å‰åçš„æ—¥å¿—
    logger.debug(f"Before filter - News count: {len(news_data)}")
    
    # è¿‡æ»¤æ— æ•ˆé“¾æ¥
    invalid_links = [
        "https://travel.cnr.cn/travel.cnr.cn/mlzgtgx",
        "https://travel.cnr.cn/travel.cnr.cn/hydt/"
    ]
    
    # è¿‡æ»¤æ— æ•ˆé“¾æ¥
    filtered_news = []
    for news in news_data:
        link = news.get('link', '')
        # è·³è¿‡æ— æ•ˆé“¾æ¥
        if link in invalid_links:
            logger.debug(f"Filtering invalid link: {link}")
            continue
        filtered_news.append(news)
    news_data = filtered_news
    logger.debug(f"After invalid link filter - News count: {len(news_data)}")
    
    # å…³é”®è¯è¿‡æ»¤
    if search_text:
        logger.debug(f"Applying search filter: '{search_text}'")
        filtered_news = []
        for news in news_data:
            if search_text.lower() in str(news.get('title','')).lower():
                filtered_news.append(news)
        news_data = filtered_news
        logger.debug(f"After text filter - News count: {len(news_data)}")
    
    # è‹¥æœ‰æœç´¢è¯ï¼Œè¿›è¡Œå‘½ä¸­ä¼˜å…ˆæ’åºï¼šå®Œå…¨å‘½ä¸­ > å­ä¸²å‘½ä¸­ï¼›åŒç»„æŒ‰æ—¥æœŸå€’åº
    if search_text:
        logger.debug(f"Sorting with search priority")
        s = search_text.lower()
        def score(item):
            title = str(item.get('title',''))
            tl = title.lower()
            exact = 1 if tl == s else 0
            contains = 1 if s in tl else 0
            # æ›´é«˜çš„å…ƒç»„å°†æ’å‰ï¼ˆPythoné»˜è®¤ä»å‰åˆ°åæ¯”è¾ƒï¼‰
            return (exact, contains, str(item.get('date','')))
        news_data.sort(key=score, reverse=True)
    else:
        # é»˜è®¤æŒ‰æ—¥æœŸå€’åº
        logger.debug("Sorting by date descending")
        news_data.sort(key=lambda x: str(x.get('date','')), reverse=True)
    
    # è®°å½•å‰5æ¡æ–°é—»çš„æ ‡é¢˜
    if len(news_data) > 0:
        logger.debug("Top 5 news after filter and sort:")
        for i in range(min(5, len(news_data))):
            logger.debug(f"{i+1}. {news_data[i].get('title', 'No title')}")
    
    return news_data

@app.route('/news_content')
def news_content():
    link = request.args.get('link')
    if not link:
        return "ç¼ºå°‘é“¾æ¥å‚æ•°", 400
    website = request.args.get('website', 'äººæ°‘ç½‘æ—…æ¸¸é¢‘é“')
    search_text = request.args.get('search', '')
    
    # é‡æ–°è·å–æ–°é—»æ•°æ®ä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼Œä¸indexå‡½æ•°ä¿æŒç›¸åŒçš„é€»è¾‘
    sources_str = request.args.get('sources', '').strip()
    selected_sources = [s for s in [x.strip() for x in sources_str.split(',')] if s] if sources_str else []
    
    if selected_sources:
        # å¤šæ¥æºé€‰æ‹©
        valid_sources = [s for s in selected_sources if s in websites]
        aggregated = []
        for site in valid_sources:
            aggregated.extend(get_news_with_cache(site))
        # å»é‡å¹¶æ’åº
        seen = set()
        deduped = []
        for item in aggregated:
            item_link = item.get('link')
            if item_link and item_link not in seen:
                seen.add(item_link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        news_data = deduped
    elif website == ALL_SOURCES_LABEL:
        # å¯¹äº"å…¨éƒ¨æ¥æº"ï¼Œéœ€è¦èšåˆæ‰€æœ‰ç½‘ç«™çš„æ–°é—»æ•°æ®
        aggregated = []
        for site in websites.keys():
            aggregated.extend(get_news_with_cache(site))
        # å»é‡å¹¶æ’åº
        seen = set()
        deduped = []
        for item in aggregated:
            item_link = item.get('link')
            if item_link and item_link not in seen:
                seen.add(item_link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        news_data = deduped
    else:
        # å¯¹äºå•ä¸ªç½‘ç«™ï¼Œä½¿ç”¨ä¸indexå‡½æ•°ç›¸åŒçš„æ–¹å¼è·å–æ•°æ®
        news_data = get_news_with_cache(website)

    # å»é‡ï¼ˆæŒ‰é“¾æ¥ï¼‰ï¼Œå³ä½¿æ˜¯å•ä¸ªç½‘ç«™ä¹Ÿå¯èƒ½æœ‰é‡å¤
    seen = set()
    deduped = []
    for item in news_data:
        item_link = item.get('link')
        if item_link and item_link not in seen:
            seen.add(item_link)
            deduped.append(item)
    news_data = deduped

    # åº”ç”¨ä¸indexå‡½æ•°ç›¸åŒçš„è¿‡æ»¤é€»è¾‘
    news_data = filter_news(news_data, search_text)
    
    # æ”¯æŒåˆ†é¡µå‚æ•°
    page = request.args.get('page', '1')
    page_size = request.args.get('page_size', '12')
    try:
        page = int(page)
        page_size = int(page_size)
        page = max(page, 1)
        page_size = max(min(page_size, 30), 6)
    except Exception:
        page, page_size = 1, 12
        
    # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹å’Œç»“æŸç´¢å¼•
    start_index = (page - 1) * page_size
    end_index = start_index + page_size
    
    # æ ¹æ®åˆ†é¡µå‚æ•°æˆªå–å½“å‰é¡µçš„æ–°é—»æ•°æ®
    paged_news_data = news_data[start_index:end_index]
    
    # Find the news item by link
    print(f"\n=== NEWS CONTENT DEBUG INFO ===")
    print(f"Searching for news with link: {link}")
    print(f"Total news items after filtering: {len(news_data)}")
    print(f"Current page: {page}, page size: {page_size}")
    print(f"Page range: {start_index}-{end_index}")
    print(f"News items on current page: {len(paged_news_data)}")
    print(f"=== END DEBUG INFO ===\n")
    
    news_item = None
    global_index = None
    
    # ç›´æ¥åœ¨å®Œæ•´åˆ—è¡¨ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–°é—»
    for i, item in enumerate(news_data):
        item_link = item.get('link')
        if item_link == link:
            logger.info(f"Found matching news at index {i}: {item.get('title', 'No title')}")
            news_item = item.copy()
            global_index = i
            # è®¡ç®—è¿™æ¡æ–°é—»åº”è¯¥åœ¨ç¬¬å‡ é¡µ
            page = (i // page_size) + 1
            # æ›´æ–°åˆ†é¡µèŒƒå›´
            start_index = (page - 1) * page_size
            end_index = start_index + page_size
            paged_news_data = news_data[start_index:end_index]
            break
    
    if news_item is None:
        print(f"Warning: News not found for link: {link}")
        return "æ–°é—»ä¸å­˜åœ¨", 404

    content = get_news_content(link, news_item['source'])
    news_item['content'] = content
    
    # è®¡ç®—å‰åé“¾æ¥
    prev_link = None
    next_link = None
    
    if global_index is not None:
        # ä½¿ç”¨å…¨å±€ç´¢å¼•è®¡ç®—å‰åé“¾æ¥ï¼Œè¿™æ˜¯æœ€å¯é çš„æ–¹æ³•
        if global_index > 0:
            prev_link = news_data[global_index - 1]['link']
            print(f"Using global index {global_index} to find previous link: {prev_link}")
        else:
            print(f"No previous link (global index is 0)")
        
        if global_index + 1 < len(news_data):
            next_link = news_data[global_index + 1]['link']
            print(f"Using global index {global_index} to find next link: {next_link}")
        else:
            print(f"No next link (global index is at end)")
    
    return render_template('news_content.html', 
                          news_item=news_item, 
                          websites=list(websites.keys()),
                          current_website=website,
                          prev_link=prev_link,
                          next_link=next_link,
                          page=page,
                          page_size=page_size,
                          sources=sources_str,
                          search=search_text)

@app.route('/fetch_news')
def api_fetch_news():
    website = request.args.get('website', 'äººæ°‘ç½‘æ—…æ¸¸é¢‘é“')
    refresh = request.args.get('refresh', '0') == '1'
    search_text = request.args.get('search', '')
    sources_str = request.args.get('sources', '').strip()
    selected_sources = [s for s in [x.strip() for x in sources_str.split(',')] if s] if sources_str else []
    if selected_sources:
        valid_sources = [s for s in selected_sources if s in websites]
        aggregated = []
        for site in valid_sources:
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        news_data = filter_news(deduped, search_text)
    elif website == ALL_SOURCES_LABEL:
        aggregated = []
        for site in websites.keys():
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        news_data = filter_news(deduped, search_text)
    else:
        news_data = filter_news(get_news_with_cache(website, force_refresh=refresh), search_text)
    return jsonify(news_data)

@app.route('/export')
def export_data():
    website = request.args.get('website', ALL_SOURCES_LABEL)
    fmt = request.args.get('format', 'json').lower()
    refresh = request.args.get('refresh', '0') == '1'
    search_text = request.args.get('search', '')
    sources_str = request.args.get('sources', '').strip()
    selected_sources = [s for s in [x.strip() for x in sources_str.split(',')] if s] if sources_str else []
    # å¤ç”¨èšåˆé€»è¾‘
    if selected_sources:
        valid_sources = [s for s in selected_sources if s in websites]
        aggregated = []
        for site in valid_sources:
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        data = filter_news(deduped, search_text)
    elif website == ALL_SOURCES_LABEL:
        aggregated = []
        for site in websites.keys():
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        data = filter_news(deduped, search_text)
    else:
        data = filter_news(get_news_with_cache(website, force_refresh=refresh), search_text)

    if fmt == 'csv':
        output = io.StringIO()
        writer = csv.DictWriter(output, fieldnames=['title', 'link', 'source', 'date'])
        writer.writeheader()
        for row in data:
            writer.writerow({
                'title': row.get('title', ''),
                'link': row.get('link', ''),
                'source': row.get('source', ''),
                'date': row.get('date', ''),
            })
        resp = Response(output.getvalue(), mimetype='text/csv; charset=utf-8')
        resp.headers['Content-Disposition'] = 'attachment; filename="news.csv"'
        return resp
    else:
        return jsonify(data)

@app.route('/feed.xml')
def rss_feed():
    website = request.args.get('website', ALL_SOURCES_LABEL)
    refresh = request.args.get('refresh', '0') == '1'
    search_text = request.args.get('search', '')
    sources_str = request.args.get('sources', '').strip()
    selected_sources = [s for s in [x.strip() for x in sources_str.split(',')] if s] if sources_str else []

    # æ•°æ®è·å–ä¸ç­›é€‰ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
    if selected_sources:
        valid_sources = [s for s in selected_sources if s in websites]
        aggregated = []
        for site in valid_sources:
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        data = filter_news(deduped, search_text)
    elif website == ALL_SOURCES_LABEL:
        aggregated = []
        for site in websites.keys():
            aggregated.extend(get_news_with_cache(site, force_refresh=refresh))
        seen = set()
        deduped = []
        for item in aggregated:
            link = item.get('link')
            if link and link not in seen:
                seen.add(link)
                deduped.append(item)
        deduped.sort(key=lambda x: x.get('date', ''), reverse=True)
        data = filter_news(deduped, search_text)
    else:
        data = filter_news(get_news_with_cache(website, force_refresh=refresh), search_text)

    # ç”Ÿæˆ RSS 2.0
    base = request.url_root.rstrip('/')
    title = f"å›½å†…æ—…æ¸¸èµ„è®¯ - {website if not selected_sources else ','.join(selected_sources)}"
    feed_items = []
    for item in data[:50]:
        t = (item.get('title') or '').replace('&', '&amp;')
        l = (item.get('link') or '').replace('&', '&amp;')
        d = (item.get('date') or '')
        feed_items.append(f"""
    <item>
      <title>{t}</title>
      <link>{l}</link>
      <guid isPermaLink="true">{l}</guid>
      <pubDate>{d}</pubDate>
      <description><![CDATA[{t}]]></description>
    </item>
""")
    rss = f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>{title}</title>
    <link>{base}/</link>
    <description>æŒ‰å½“å‰ç­›é€‰ç”Ÿæˆçš„èµ„è®¯è®¢é˜…</description>
    {''.join(feed_items)}
  </channel>
</rss>
"""
    return Response(rss, mimetype='application/rss+xml; charset=utf-8')

def get_news_with_cache(website, force_refresh=False):
    now = time.time()
    cache_entry = _cache.get(website)
    if not force_refresh and cache_entry and (now - cache_entry["ts"]) < CACHE_TTL_SECONDS:
        return cache_entry["data"]
    t0 = time.time()
    data = fetch_news(website)
    t1 = time.time()
    _cache[website] = {"data": data, "ts": now}
    # è®°å½•æŒ‡æ ‡
    _metrics['last_fetch'][website] = {
        'ts': int(now),
        'duration_ms': int((t1 - t0) * 1000),
        'count': len(data),
    }
    return data

def get_default_headers():
    return {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
        ),
        "Accept": (
            "text/html,application/xhtml+xml,application/xml;q=0.9,"
            "image/avif,image/webp,*/*;q=0.8"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
    }

def fetch_url(url, headers=None, timeout=10):
    """ç»Ÿä¸€çš„HTTPè·å–å‡½æ•°ï¼›å¦‚è®¾ç½®äº†PROXY_BASEåˆ™ç»ç”±ä»£ç†ã€‚

    PROXY_BASE ç¤ºä¾‹: https://your-worker-subdomain.workers.dev
    å®é™…è¯·æ±‚ä¸º: PROXY_BASE + '?url=' + urlencode(original_url)
    """
    proxy_base = os.environ.get('PROXY_BASE', '').strip()
    target = url
    if proxy_base:
        # ç¡®ä¿æœ«å°¾æ²¡æœ‰å¤šä½™æ–œæ ï¼Œé¿å… //
        proxy_base = proxy_base.rstrip('/')
        target = f"{proxy_base}?url={quote_plus(url)}"
    return requests.get(target, headers=headers, timeout=timeout)

def fetch_url_with_retries(url, headers=None, timeout=10, retries=3):
    last_exc = None
    for attempt in range(retries):
        try:
            resp = fetch_url(url, headers=headers or get_default_headers(), timeout=timeout)
            if resp.status_code == 200 and resp.text:
                return resp
            # å¯¹äºéå¸¸çŸ­çš„å“åº”æˆ–ä¸´æ—¶é”™è¯¯ï¼Œè§¦å‘é‡è¯•
        except Exception as exc:
            last_exc = exc
        # æŒ‡æ•°é€€é¿ + è½»å¾®æŠ–åŠ¨
        backoff_ms = (2 ** attempt) * 0.3 + random.uniform(0.05, 0.2)
        time.sleep(backoff_ms)
    # æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºæˆ–è¿”å›å ä½å“åº”
    if last_exc:
        raise last_exc
    return fetch_url(url, headers=headers or get_default_headers(), timeout=timeout)

def fetch_news(website):
    """è·å–é€‰å®šç½‘ç«™çš„æœ€æ–°èµ„è®¯"""
    website_config = websites.get(website)
    if not website_config:
        return []
    
    url = website_config["url"]
    link_css = website_config["link_css"]  # ä½¿ç”¨CSSé€‰æ‹©å™¨æ›¿ä»£XPath
    base_url = website_config["base_url"]
    
    # å­˜å‚¨å¤„ç†åçš„é“¾æ¥é›†åˆï¼Œç”¨äºå»é‡
    processed_links = set()
    news_data = []
    
    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        headers = get_default_headers()
        
        # å‘é€è¯·æ±‚ï¼ˆå¯¹ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘å¢åŠ å›é€€URLä»¥æå‡æˆåŠŸç‡ï¼‰
        candidate_urls = [url]
        if website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            candidate_urls.extend([
                "https://www.ctnews.com.cn/jujiao/",  # æ ç›®é¡µ
                "https://www.ctnews.com.cn/",         # é¦–é¡µ
            ])
        response = None
        for cand in candidate_urls:
            try:
                response = fetch_url_with_retries(cand, headers=headers, timeout=10, retries=3)
                if response and response.status_code == 200 and response.text and len(response.text) > 1000:
                    print(f"æˆåŠŸæŠ“å– {website} ä» {cand}, å†…å®¹é•¿åº¦: {len(response.text)}")
                    break
                else:
                    print(f"æŠ“å–å¤±è´¥ {website} ä» {cand}, çŠ¶æ€ç : {response.status_code if response else 'None'}")
            except Exception as e:
                print(f"æŠ“å–å¼‚å¸¸ {website} ä» {cand}: {str(e)}")
                response = None
                continue
        if response is None:
            print(f"æ‰€æœ‰URLéƒ½å¤±è´¥ {website}")
            return []
        
        # æ ¹æ®ä¸åŒç½‘ç«™è®¾ç½®ä¸åŒçš„ç¼–ç ç­–ç•¥
        if website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
            # å¤®å¹¿ç½‘å¯èƒ½ä½¿ç”¨GBKç¼–ç 
            try:
                # å…ˆå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                response.encoding = response.apparent_encoding
                
                # æµ‹è¯•è§£ç æ˜¯å¦æ­£å¸¸
                test_content = response.text
                # å¦‚æœæœ‰ä¹±ç ç‰¹å¾ï¼Œå°è¯•GBKç¼–ç 
                if any(ch in test_content for ch in ['é”Ÿæ–¤æ‹·', 'çƒ«çƒ«çƒ«', '????']):
                    response.encoding = "gbk"
            except Exception:
                response.encoding = "gbk"  # é»˜è®¤ä¸ºGBK
        elif website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            # ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘ä½¿ç”¨UTF-8ç¼–ç 
            response.encoding = "utf-8"
        else:
            # å…¶ä»–ç½‘ç«™ä½¿ç”¨UTF-8æˆ–è‡ªåŠ¨æ£€æµ‹
            response.encoding = response.apparent_encoding
        
        if response.status_code != 200:
            return []
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è·å–æ ‡é¢˜å’Œé“¾æ¥ - æ”¯æŒå•ä¸ªé€‰æ‹©å™¨æˆ–é€‰æ‹©å™¨åˆ—è¡¨
        selectors = []
        if isinstance(link_css, list):
            selectors.extend(link_css)
        else:
            selectors.append(link_css)

        # æ ¹æ®ç«™ç‚¹è¿½åŠ å¤‡ç”¨é€‰æ‹©å™¨ä»¥æå‡æˆåŠŸç‡
        if website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
            selectors.extend([
                'a[href*="/n1/"]',
                'div.ej_list_box a',
                'div.box a',
                'div.list_box a',
            ])
        if website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            selectors.extend([
                'a[href*="/content/"]',
                'div.list a[href*="/content/"]',
                'div.article-list a[href*="/content/"]',
                'ul li a[href*="/content/"]',
                'section a[href*="/content/"]',
            ])

        news_items = []
        for css in selectors:
            try:
                css_links = soup.select(css)
                if css_links:
                    print(f"é€‰æ‹©å™¨ {css} åŒ¹é…åˆ° {len(css_links)} ä¸ªé“¾æ¥")
                    news_items.extend(css_links)
                else:
                    print(f"é€‰æ‹©å™¨ {css} æœªåŒ¹é…åˆ°é“¾æ¥")
            except Exception as e:
                print(f"é€‰æ‹©å™¨ {css} å¼‚å¸¸: {str(e)}")
                continue
        
        print(f"æ€»å…±æ‰¾åˆ° {len(news_items)} ä¸ªé“¾æ¥å…ƒç´ ")
        
        # å»é‡å¤„ç†ï¼Œä¼˜å…ˆä¿ç•™æœ‰æ–‡æœ¬å†…å®¹çš„é“¾æ¥
        seen_hrefs = {}
        for item in news_items:
            href = item.get('href', '')
            if href:
                text = item.text.strip() if item.text else ""
                # å¦‚æœè¿™ä¸ªé“¾æ¥è¿˜æ²¡æœ‰è®°å½•ï¼Œæˆ–è€…æ–°é“¾æ¥æœ‰æ–‡æœ¬è€Œæ—§é“¾æ¥æ²¡æœ‰æ–‡æœ¬
                if href not in seen_hrefs or (text and not seen_hrefs[href].text.strip()):
                    seen_hrefs[href] = item
        
        news_items = list(seen_hrefs.values())
        
        # æ ¹æ®ä¸åŒç½‘ç«™è°ƒæ•´è¿‡æ»¤ç­–ç•¥
        if website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            filter_words = ['æ— æ ‡é¢˜', 'è¯¦æƒ…', 'è§†é¢‘', 'å›¾ç‰‡', 'æ›´å¤š', 'ç›¸å…³', 'æ¨è']
        elif website == "ç¯çƒç½‘æ—…æ¸¸é¢‘é“":
            filter_words = ['æ— æ ‡é¢˜', 'è¯¦æƒ…', 'è§†é¢‘', 'å›¾ç‰‡', 'æ›´å¤š', 'ç›¸å…³', 'æ¨è', 'é¦–é¡µ', 'è¿”å›']
        elif website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
            filter_words = ['æ— æ ‡é¢˜', 'è¯¦æƒ…', 'è§†é¢‘', 'å›¾ç‰‡', 'æ›´å¤š', 'ç›¸å…³', 'æ¨è', 'é¦–é¡µ']
        else:  # å…¶ä»–ç½‘ç«™
            filter_words = ['æ— æ ‡é¢˜', 'è¯¦æƒ…', 'è§†é¢‘', 'å›¾ç‰‡', 'æ›´å¤š', 'ç›¸å…³', 'æ¨è', 'é¦–é¡µ']
        
        for item in news_items:
            title = ""
            link = ""
            
            # æ£€æŸ¥itemçš„ç±»å‹
            if isinstance(item, str):
                link = item.strip()
                # å°è¯•ä»URLä¸­æå–æ ‡é¢˜ä¿¡æ¯
                if link:
                    try:
                        # é€‚é…äººæ°‘ç½‘çš„URLæ ¼å¼
                        title_match = re.search(r'/(\d{4})/(\d{2})(\d{2})/([\w-]+)\.html', link)
                        if title_match:
                            title = title_match.group(4)  # ä½¿ç”¨URLä¸­çš„æœ€åéƒ¨åˆ†ä½œä¸ºä¸´æ—¶æ ‡é¢˜
                    except:
                        pass
            else:
                # åŸæœ‰çš„å…ƒç´ èŠ‚ç‚¹å¤„ç†é€»è¾‘
                title = item.text.strip() if item.text else ""
                
                # ç¯çƒç½‘ç‰¹å®šçš„æ ‡é¢˜æå–é€»è¾‘
                if website == "ç¯çƒç½‘æ—…æ¸¸é¢‘é“" and not title:
                    # å°è¯•è·å–altå±æ€§
                    title = item.get('alt', '').strip()
                    # å°è¯•è·å–titleå±æ€§
                    if not title:
                        title = item.get('title', '').strip()
                    # å°è¯•è·å–çˆ¶å…ƒç´ çš„æ–‡æœ¬
                    if not title:
                        parent = item.findparent()
                        if parent and parent.text:
                            title = parent.text.strip()
                    # å°è¯•è·å–å…„å¼Ÿå…ƒç´ çš„æ–‡æœ¬
                    if not title:
                        next_sibling = item.getnext()
                        if next_sibling and next_sibling.text:
                            title = next_sibling.text.strip()
                    # å°è¯•è·å–å­å…ƒç´ çš„æ–‡æœ¬
                    if not title:
                        for child in item.iter():
                            if child.text and len(child.text.strip()) > 0:
                                title = child.text.strip()
                                break
                
                link = item.get('href') if item.get('href') else ""
            
            # è·³è¿‡æ— æ•ˆæ ‡é¢˜å’Œé“¾æ¥
            if not title or len(title) < 5 or not link:
                print(f"è·³è¿‡æ— æ•ˆé¡¹: æ ‡é¢˜='{title[:30]}', é“¾æ¥='{link[:50]}'")
                continue
            
            # ç¡®ä¿é“¾æ¥ä¸æ˜¯None
            if link is None:
                continue
            
            # å°†é“¾æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            link = str(link)
            
            # æ¸…ç†é“¾æ¥ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            link = link.strip()
            
            # ä¿®å¤å¯èƒ½çš„é‡å¤base_urlé—®é¢˜
            try:
                # å¤„ç†å¤šç§å¯èƒ½çš„é‡å¤æ ¼å¼
                if isinstance(link, str) and isinstance(base_url, str) and link.startswith(base_url + base_url):
                    link = link.replace(base_url + base_url, base_url)
                # å¤„ç†å¯èƒ½çš„åŒæ–œæ é—®é¢˜
                if '//' in link and link.startswith('http'):
                    parts = link.split('//')
                    if len(parts) > 2:
                        # ä¿ç•™åè®®éƒ¨åˆ†ï¼Œåˆå¹¶åé¢çš„éƒ¨åˆ†
                        link = parts[0] + '//' + '/'.join(parts[1:])
            except Exception:
                pass
            
            # æ ¹æ®ä¸åŒç½‘ç«™è¿›è¡Œç‰¹å®šçš„é“¾æ¥è¿‡æ»¤
            if website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
                # å¤®å¹¿ç½‘ç‰¹å®šè¿‡æ»¤
                if "javascript" in link.lower() or "#" in link:
                    continue
                # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹é“¾æ¥ï¼Œå¦‚æœä¸æ˜¯åˆ™æ·»åŠ åŸºç¡€URL
                if not link.startswith('http'):
                    if link.startswith('/'):
                        link = f"https://travel.cnr.cn{link}"
                    else:
                        link = f"https://travel.cnr.cn/{link}"
                # ä¿®å¤åŒæ–œæ é—®é¢˜ï¼šåªä¿®å¤åè®®åçš„åŒæ–œæ ï¼Œä¿ç•™åè®®æœ¬èº«
                if link.startswith('http'):
                    # å°†åè®®åçš„åŒæ–œæ æ›¿æ¢ä¸ºå•æ–œæ 
                    protocol_end = link.find('//') + 2
                    protocol_part = link[:protocol_end]
                    path_part = link[protocol_end:]
                    # ä¿®å¤è·¯å¾„ä¸­çš„åŒæ–œæ 
                    path_part = path_part.replace('//', '/')
                    link = protocol_part + path_part
                
                # è¿‡æ»¤æ˜æ˜¾æ— æ•ˆçš„å¤®å¹¿ç½‘é“¾æ¥æ¨¡å¼ï¼ˆæ›´ç²¾ç¡®çš„è¿‡æ»¤ï¼‰
                invalid_cnr_patterns = [
                    '/cnr_404/',      # ç›´æ¥404é¡µé¢
                    '//cnr.cn/',      # è·¨åŸŸé“¾æ¥
                    '/2024zt/ai/',    # ç‰¹å®šçš„AIä¸“é¢˜é¡µé¢ï¼ˆå·²çŸ¥404ï¼‰
                    '/news.cnr.cn/2024zt/',  # è·¨åŸŸä¸“é¢˜é¡µé¢
                    '/www.cnr.cn/2024zt/'    # è·¨åŸŸä¸“é¢˜é¡µé¢
                ]
                
                if any(pattern in link for pattern in invalid_cnr_patterns):
                    print(f"è¿‡æ»¤æ— æ•ˆå¤®å¹¿ç½‘é“¾æ¥: {link}")
                    continue
            elif website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
                # äººæ°‘ç½‘æ—…æ¸¸é¢‘é“ç‰¹å®šè¿‡æ»¤
                if "javascript" in link.lower() or "#" in link:
                    continue
            
            # è·³è¿‡åŒ…å«è¿‡æ»¤å…³é”®è¯çš„æ ‡é¢˜
            if any(word in title for word in filter_words):
                continue
            
            # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„
            if not link.startswith('http'):
                # æ ¹æ®ä¸åŒç½‘ç«™å¤„ç†ç›¸å¯¹é“¾æ¥
                if website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
                    link = f"https://www.ctnews.com.cn{link}"
                elif website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
                    # äººæ°‘ç½‘æ—…æ¸¸é¢‘é“ç›¸å¯¹é“¾æ¥å¤„ç†
                    if link.startswith('/'):
                        link = f"http://travel.people.com.cn{link}"
                    else:
                        link = f"http://travel.people.com.cn/{link}"
                else:  # å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“
                      if link.startswith('/'):
                          link = f"https://travel.cnr.cn{link}"
                      else:
                          link = f"https://travel.cnr.cn/{link}"
            
            # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²å¤„ç†è¿‡ï¼ˆé˜²æ­¢å†…å®¹é‡å¤ï¼‰
            if link in processed_links:
                continue
            
            # æ·»åŠ é“¾æ¥åˆ°å·²å¤„ç†é›†åˆ
            processed_links.add(link)
            
            # ä»é“¾æ¥ä¸­æå–æ—¥æœŸä¿¡æ¯æˆ–è®¾ç½®é»˜è®¤æ—¥æœŸ
            date_str = extract_date_from_link(link, website)
            
            # ä¿å­˜æ–°é—»æ•°æ®
            news_data.append({
                'title': title,
                'link': link,
                'source': website,
                'date': date_str
            })
            
            # é™åˆ¶æ–°é—»æ•°é‡ï¼Œé˜²æ­¢è¿‡å¤š
            if len(news_data) >= 50:
                break
        
        print(f"å¤„ç†å®Œæˆåå¾—åˆ° {len(news_data)} æ¡æ–°é—»")
        
    except Exception as e:
        error_msg = str(e)
        print(f"è·å–èµ„è®¯å¤±è´¥: {error_msg}")
        try:
            _errors.append({
                'ts': int(time.time()),
                'website': website,
                'stage': 'fetch_news',
                'message': error_msg,
            })
        except Exception:
            pass
        # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å´©æºƒ
        return []
        
    # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    news_data.sort(key=lambda x: x['date'], reverse=True)
    
    print(f"æœ€ç»ˆè¿”å› {len(news_data)} æ¡æ–°é—»")
    return news_data

def extract_date_from_link(link, website):
    """ä»é“¾æ¥ä¸­æå–æ—¥æœŸä¿¡æ¯"""
    try:
        # äººæ°‘ç½‘æ—…æ¸¸é¢‘é“ URLæ ¼å¼: http://travel.people.com.cn/n1/2024/0925/c41570-40567332.html
        if website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
            date_match = re.search(r'/n1/(\d{4})/(\d{2})(\d{2})/', link)
            if date_match:
                year = date_match.group(1)
                month = date_match.group(2)
                day = date_match.group(3)
                return f"{year}-{month}-{day}"
        # ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘ URLæ ¼å¼: https://www.ctnews.com.cn/content/2024-09/25/content_13044816.htm
        elif website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            date_match = re.search(r'/content/(\d{4}-\d{2})/(\d{2})/', link)
            if date_match:
                return f"{date_match.group(1)}-{date_match.group(2)}"
        # å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“ URLæ ¼å¼å¯èƒ½ä¸ç»Ÿä¸€ï¼Œå°è¯•é€šç”¨æå–
        elif website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
            # å°è¯•ä»é“¾æ¥ä¸­æå–ä»»ä½•å¯èƒ½çš„æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})-(\d{2})-(\d{2})',
                r'(\d{4})/(\d{2})/(\d{2})',
                r'(\d{4})(\d{2})(\d{2})'
            ]
            for pattern in date_patterns:
                date_match = re.search(pattern, link)
                if date_match and len(date_match.group(1)) == 4:
                    if len(date_match.groups()) == 3:
                        return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
        
        # å¦‚æœæ— æ³•ä»é“¾æ¥æå–æ—¥æœŸï¼Œè¿”å›å½“å‰æ—¥æœŸ
        return datetime.now().strftime('%Y-%m-%d')
    except Exception:
        # å‡ºé”™æ—¶è¿”å›å½“å‰æ—¥æœŸ
        return datetime.now().strftime('%Y-%m-%d')

def get_news_content(link, website):
    """è·å–æ–°é—»çš„è¯¦ç»†å†…å®¹"""
    try:
        # éªŒè¯é“¾æ¥æ ¼å¼ï¼Œè¿‡æ»¤æ— æ•ˆé“¾æ¥
        if not link or not isinstance(link, str) or len(link.strip()) < 10:
            return "é“¾æ¥æ ¼å¼æ— æ•ˆ"
        
        # ä¿®å¤é“¾æ¥ä¸­çš„åŒæ–œæ é—®é¢˜
        link = link.replace('//', '/').replace('https:/', 'https://').replace('http:/', 'http://')
        
        # è¿‡æ»¤æ˜æ˜¾æ— æ•ˆçš„é“¾æ¥æ¨¡å¼
        invalid_patterns = [
            'javascript:', '#', 'mailto:', 'tel:', 'ftp:', 'file:',
            '//news.cnr.cn//',  # å¤®å¹¿ç½‘ç‰¹å®šçš„æ— æ•ˆé“¾æ¥æ¨¡å¼
            '//travel.cnr.cn//'  # å¤®å¹¿ç½‘ç‰¹å®šçš„æ— æ•ˆé“¾æ¥æ¨¡å¼
        ]
        
        if any(pattern in link for pattern in invalid_patterns):
            return "é“¾æ¥æ ¼å¼æ— æ•ˆ"
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6"
        }
        
        # è¯·æ±‚æ–°é—»è¯¦æƒ…é¡µ
        response = fetch_url_with_retries(link, headers=headers, timeout=10, retries=3)
        
        # æ£€æŸ¥å“åº”çŠ¶æ€ï¼Œè¿‡æ»¤404ç­‰é”™è¯¯é¡µé¢
        if response.status_code != 200:
            if response.status_code == 404:
                return "é¡µé¢ä¸å­˜åœ¨(404)"
            elif response.status_code >= 400:
                return f"é¡µé¢è®¿é—®å¤±è´¥({response.status_code})"
        
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è§£å†³ä¹±ç é—®é¢˜
        if website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
            # å¤®å¹¿ç½‘è¯¦æƒ…é¡µç¼–ç å¤„ç†
            try:
                # å…ˆå°è¯•GBKç¼–ç ï¼Œè¿™æ˜¯å¤®å¹¿ç½‘å¸¸ç”¨çš„ç¼–ç 
                response.encoding = "gbk"
                test_content = response.text
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¹±ç ç‰¹å¾
                if any(ch in test_content for ch in ['é”Ÿæ–¤æ‹·', 'çƒ«çƒ«çƒ«', '????']):
                    response.encoding = response.apparent_encoding
            except Exception:
                response.encoding = "utf-8"  # é»˜è®¤ä½¿ç”¨UTF-8
        else:
            # å…¶ä»–ç½‘ç«™ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            response.encoding = response.apparent_encoding
        
        # è§£æå†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ ¹æ®ä¸åŒç½‘ç«™ä½¿ç”¨ä¸åŒçš„å†…å®¹æå–ç­–ç•¥
        main_paragraphs = []
        
        # 1. æŸ¥æ‰¾ä¸»è¦å†…å®¹å®¹å™¨ - ä½¿ç”¨ç½‘ç«™ç‰¹å®šç­–ç•¥
        main_content = None
        if website == "ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘":
            # ä¸­å›½æ—…æ¸¸æ–°é—»ç½‘ç‰¹å®šå†…å®¹æå–
            main_content = soup.select_one('div.article-content, div#article_body, div.content, div.main-content, div.article-body')
        elif website == "äººæ°‘ç½‘æ—…æ¸¸é¢‘é“":
            # äººæ°‘ç½‘æ—…æ¸¸é¢‘é“ç‰¹å®šå†…å®¹æå–
            main_content = soup.select_one('div#rwb_zw, div#articleText, div.rm_txt_con, div.article-content, div.content, div.main')
        elif website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
            # å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“ç‰¹å®šå†…å®¹æå–
            main_content = soup.select_one('div.article-content, div.content, div.main, div.article-body, div.article-text')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå†…å®¹ï¼Œå°è¯•é€šç”¨é€‰æ‹©å™¨
        if not main_content:
            main_content = soup.select_one('div.content, div#article, div.article-content, div.content-main, div.main-content, div.article-body, div.article-text, div.text-content')
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
        if not main_content:
            # æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„å®¹å™¨
            content_candidates = soup.select('div, article, section')
            for candidate in content_candidates:
                text_length = len(candidate.get_text(strip=True))
                if text_length > 200:  # å‡è®¾ä¸»è¦å†…å®¹è‡³å°‘200å­—ç¬¦
                    main_content = candidate
                    break
        
        if main_content:
            # 2. é¦–å…ˆå°è¯•æå–pæ ‡ç­¾å†…å®¹
            paragraphs = main_content.find_all('p')
            
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 5:
                    main_paragraphs.append(text)
            
            # å¦‚æœpæ ‡ç­¾å†…å®¹ä¸è¶³ï¼Œå†å°è¯•æå–å…¶ä»–æ ‡ç­¾å†…å®¹
            if len(main_paragraphs) < 3:
                # å°è¯•æå–divæ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
                divs = main_content.find_all('div')
                for div in divs:
                    text = div.get_text(strip=True)
                    if text and len(text) > 5 and text not in main_paragraphs:
                        main_paragraphs.append(text)
                
                # å°è¯•æå–spanæ ‡ç­¾å†…å®¹
                spans = main_content.find_all('span')
                for span in spans:
                    text = span.get_text(strip=True)
                    if text and len(text) > 5 and text not in main_paragraphs:
                        main_paragraphs.append(text)
                
                # å°è¯•æå–sectionæ ‡ç­¾å†…å®¹
                sections = main_content.find_all('section')
                for section in sections:
                    text = section.get_text(strip=True)
                    if text and len(text) > 5 and text not in main_paragraphs:
                        main_paragraphs.append(text)
                
                # å°è¯•æå–articleæ ‡ç­¾å†…å®¹
                articles = main_content.find_all('article')
                for article in articles:
                    text = article.get_text(strip=True)
                    if text and len(text) > 5 and text not in main_paragraphs:
                        main_paragraphs.append(text)
        
        # å¦‚æœæ‰¾åˆ°æ­£æ–‡æ®µè½ï¼Œè¿”å›å®ƒä»¬
        if main_paragraphs:
            return "\n\n".join(main_paragraphs)
        else:
            # å¤‡ç”¨ç­–ç•¥ï¼šä½¿ç”¨ç½‘ç«™ç‰¹å®šçš„å¤‡ç”¨æå–ç­–ç•¥
            if website == "ç¯çƒç½‘æ—…æ¸¸é¢‘é“":
                # ç¯çƒç½‘å¤‡ç”¨ç­–ç•¥ - å°è¯•æ›´å¤šçš„å®¹å™¨é€‰æ‹©å™¨
                content_elements = soup.select('div[class*="article"], div[id*="content"], div[class*="main"], div[class*="text-main"]')
            elif website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“":
                # å¤®å¹¿ç½‘å¤‡ç”¨ç­–ç•¥ - å°è¯•æ›´å¤šçš„å®¹å™¨é€‰æ‹©å™¨
                content_elements = soup.select('div[class*="article"], div[id*="article"], div[class*="article-body"], div[class*="article-content"]')
            else:
                # é€šç”¨å¤‡ç”¨ç­–ç•¥
                content_elements = soup.select('div[class*="article-content"], div[class*="content-main"], div[id*="content"], div[class*="content"]')
            
            all_text = "\n\n".join([elem.get_text(separator='\n', strip=True) for elem in content_elements])
            
            # æ¸…ç†å¯èƒ½çš„ä¹±ç 
            if website == "å¤®å¹¿ç½‘æ–‡æ—…é¢‘é“" and all_text:
                # å°è¯•æ›¿æ¢å¯èƒ½çš„ä¹±ç å­—ç¬¦
                all_text = all_text.replace("é”Ÿæ–¤æ‹·", "").replace("çƒ«çƒ«çƒ«", "").strip()
            
            if all_text and len(all_text.strip()) > 20:
                return all_text
            else:
                return "æ— æ³•æå–å†…å®¹ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹å®Œæ•´å†…å®¹ã€‚"
        
    except Exception as e:
        return f"åŠ è½½å†…å®¹å¤±è´¥: {str(e)}"

@app.route('/healthz')
def healthz():
    return jsonify({"status": "ok", "proxy": bool(os.environ.get('PROXY_BASE'))})

@app.route('/favicon.ico')
def favicon():
    # é¿å…æ—¥å¿—é‡Œå‡ºç°å¯¹faviconçš„é”™è¯¯è¯·æ±‚
    return ('', 204)

@app.route('/metrics')
def metrics():
    if not _check_admin_key():
        return jsonify({'error': 'unauthorized'}), 401
    return jsonify(_metrics)

@app.route('/logs')
def logs():
    if not _check_admin_key():
        return jsonify({'error': 'unauthorized'}), 401
    limit = int(request.args.get('limit', '100'))
    return jsonify(_errors[-limit:])

@app.route('/clear_cache', methods=['POST', 'GET'])
def clear_cache():
    if not _check_admin_key():
        return jsonify({'error': 'unauthorized'}), 401
    _cache.clear()
    return jsonify({'ok': True})

# --- UI helpers ---
@app.template_filter('highlight')
def highlight_filter(text, keyword):
    if not text or not keyword:
        return text
    try:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        return Markup(pattern.sub(lambda m: f'<mark>{m.group(0)}</mark>', str(text)))
    except Exception:
        return text

# --- SEO ---
@app.route('/robots.txt')
def robots_txt():
    base = request.url_root.rstrip('/')
    body = f"""User-agent: *
Allow: /
Sitemap: {base}/sitemap.xml
"""
    return Response(body, mimetype='text/plain; charset=utf-8')

@app.route('/sitemap.xml')
def sitemap_xml():
    base = request.url_root.rstrip('/')
    urls = [
        f"{base}/",
        f"{base}/?website=å…¨éƒ¨æ¥æº",
    ]
    for site in websites.keys():
        urls.append(f"{base}/?website={site}")
    xml_urls = "".join([f"<url><loc>{u}</loc></url>" for u in urls])
    xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{xml_urls}
</urlset>"""
    return Response(xml, mimetype='application/xml; charset=utf-8')
# Vercelé€‚é…ï¼šæ ‡å‡†Flaskåº”ç”¨å…¥å£ç‚¹
if __name__ == '__main__':
    # ç¡®ä¿ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    # æœ¬åœ°è¿è¡Œå…¥å£ç‚¹
    print("Running in local environment")
    app.run(debug=False, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
else:
    # Vercelç¯å¢ƒä½¿ç”¨
    application = app